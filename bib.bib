@article{Mansimov2015,
abstract = {Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.},
archivePrefix = {arXiv},
arxivId = {1511.02793},
author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
doi = {10.1088/0004-6256/150/6/203},
eprint = {1511.02793},
file = {:Users/david/Dropbox/Papers/1511.02793v2.pdf:pdf},
issn = {1538-3881},
journal = {arXiv preprint},
pages = {1--12},
title = {{Generating Images from Captions with Attention}},
url = {http://arxiv.org/abs/1511.02793},
year = {2015}
}
@article{0f810e97f6074261b823b192b8084243,
title = "ImageNet Large Scale Visual Recognition Challenge",
keywords = "Benchmark, Dataset, Large-scale, Object detection, Object recognition",
author = "Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Berg, {Alexander C.} and Li Fei-Fei",
year = "2015",
month = "12",
doi = "10.1007/s11263-015-0816-y",
volume = "115",
pages = "211--252",
journal = "International Journal of Computer Vision",
issn = "0920-5691",
publisher = "Springer Netherlands",
number = "3",
}

@article{Nguyen2016,
abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
archivePrefix = {arXiv},
arxivId = {1605.09304},
author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
eprint = {1605.09304},
file = {:Users/david/Dropbox/Papers/1605.09304v3.pdf:pdf},
journal = {arXiv},
pages = {1--29},
title = {{Synthesizing the preferred inputs for neurons in neural networks via deep generator networks}},
url = {http://arxiv.org/abs/1605.09304},
year = {2016}
}

@ARTICLE{2014arXiv1410.6460S,
   author = {{Salimans}, T. and {Kingma}, D.~P. and {Welling}, M.},
    title = "{Markov Chain Monte Carlo and Variational Inference: Bridging the Gap}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1410.6460},
 primaryClass = "stat.CO",
 keywords = {Statistics - Computation, Statistics - Machine Learning},
     year = 2014,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1410.6460S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2016arXiv160505396R,
   author = {{Reed}, S. and {Akata}, Z. and {Yan}, X. and {Logeswaran}, L. and 
	{Schiele}, B. and {Lee}, H.},
    title = "{Generative Adversarial Text to Image Synthesis}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1605.05396},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
     year = 2016,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160505396R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{DBLP:journals/corr/DosovitskiyB16,
  author    = {Alexey Dosovitskiy and
               Thomas Brox},
  title     = {Generating Images with Perceptual Similarity Metrics based on Deep
               Networks},
  journal   = {CoRR},
  volume    = {abs/1602.02644},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02644},
  timestamp = {Tue, 01 Mar 2016 17:47:25 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/DosovitskiyB16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{1206.1901,
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis
algorithm, thereby avoiding the slow exploration of the state space that results from
the diffusive behaviour of simple random-walk proposals. Though originating in
physics, Hamiltonian dynamics can be applied to most problems with continuous
state spaces by simply introducing fictitious “momentum” variables. A key to
its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories
can thus be used to define complex mappings without the need to account for a
hard-to-compute Jacobian factor — a property that can be exactly maintained
even when the dynamics is approximated by discretizing time. In this review, I
discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present
some of its variations, including using windows of states for deciding on acceptance
or rejection, computing trajectories using fast approximations, tempering during
the course of a trajectory to handle isolated modes, and short-cut methods that
prevent useless trajectories from taking much computation time.
},
  added-at = {2013-11-20T16:22:16.000+0100},
  author = {Neal, Radford M.},
  biburl = {http://www.bibsonomy.org/bibtex/233fcb1d5a6d675e371170820f0b4dbcd/giacomo.fiumara},
  citeulike-article-id = {12118398},
  interhash = {f26b3ec7720399d06a5c271916a0ee92},
  intrahash = {33fcb1d5a6d675e371170820f0b4dbcd},
  journal = {Handbook of Markov Chain Monte Carlo},
  keywords = {Markov MonteCarlo},
  pages = {113--162},
  posted-at = {2013-03-06 23:35:00},
  priority = {2},
  timestamp = {2013-11-20T16:22:16.000+0100},
  title = {{MCMC} Using {Hamiltonian} Dynamics},
  volume = 54,
  year = 2010
}

@ARTICLE{Girolami_riemannmanifold,
    author = {Mark Girolami and Ben Calderhead and Siu A. Chin},
    title = {Riemann manifold Langevin and Hamiltonian Monte Carlo methods},
    journal = {J. of the Royal Statistical Society, Series B (Methodological},
    year = {}
}

@ARTICLE{2015arXiv151106807N,
   author = {{Neelakantan}, A. and {Vilnis}, L. and {Le}, Q.~V. and {Sutskever}, I. and 
	{Kaiser}, L. and {Kurach}, K. and {Martens}, J.},
    title = "{Adding Gradient Noise Improves Learning for Very Deep Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.06807},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106807N},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2014arXiv1409.0578W,
   author = {{Whye Teh}, Y. and {Thi{\'e}ry}, A. and {Vollmer}, S.},
    title = "{Consistency and fluctuations for stochastic gradient Langevin dynamics}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1409.0578},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, 60J22, 65C40},
     year = 2014,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.0578W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{conf/icml/WellingT11,
  added-at = {2011-12-16T00:00:00.000+0100},
  author = {Welling, Max and Teh, Yee Whye},
  biburl = {http://www.bibsonomy.org/bibtex/2f5ff37ac41677e6453ed602dae088478/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2011},
  editor = {Getoor, Lise and Scheffer, Tobias},
  interhash = {5c8207b828c210fb5283c258720164ee},
  intrahash = {f5ff37ac41677e6453ed602dae088478},
  keywords = {dblp},
  pages = {681-688},
  publisher = {Omnipress},
  timestamp = {2011-12-17T11:33:26.000+0100},
  title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2011.html#WellingT11},
  year = 2011
}

% TODO NEW BIBLIOGRAPHY ADDED
%DCGAN
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:Users/david/Dropbox/Papers/1511.06434v2.pdf:pdf},
issn = {0004-6361},
journal = {arXiv},
mendeley-groups = {Researh CSC2541 Project},
pages = {1--15},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}

%CaffeNet
@article{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
file = {:Users/david/Dropbox/Papers/1408.5093v1.pdf:pdf},
isbn = {9781450330633},
issn = {10636919},
journal = {ACM International Conference on Multimedia},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
mendeley-groups = {Researh CSC2541 Project},
pages = {675--678},
pmid = {18267787},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
url = {http://arxiv.org/abs/1408.5093},
year = {2014}
}

%This is the paper from where they got the DGN generator.
@article{Dosovitskiy2016,
abstract = {Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.},
archivePrefix = {arXiv},
arxivId = {1602.02644},
author = {Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1602.02644},
file = {:Users/david/Dropbox/Papers/1602.02644v2.pdf:pdf},
journal = {arXiv preprint arXiv: 1602.02644},
keywords = {convolutional networks, generative models, image g},
mendeley-groups = {Researh CSC2541 Project},
title = {{Generating Images with Perceptual Similarity Metrics based on Deep Networks}},
url = {http://arxiv.org/abs/1602.02644},
year = {2016}
}

%ImageNet 
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia Deng Jia and Dong, Wei Dong Wei and Socher, R. and Li, Li-Jia Li Li-Jia and Li, Kai Li Kai and Fei-Fei, Li Fei-Fei Li},
doi = {10.1109/CVPR.2009.5206848},
file = {:Users/david/Dropbox/Papers/imagenet{\_}cvpr09.pdf:pdf},
isbn = {978-1-4244-3992-8},
issn = {1063-6919},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Researh CSC2541 Project},
pages = {2--9},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},
year = {2009}
}

%AlexNet and MIT PLACES DATA SET EXPERIMENT 2
@article{Zhou2014,
abstract = {Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.},
author = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
file = {:Users/david/Dropbox/Papers/places{\_}NIPS14.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
mendeley-groups = {Researh CSC2541 Project},
pages = {487--495},
title = {{Learning Deep Features for Scene Recognition using Places Database}},
url = {http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf},
year = {2014}
}

%Activation maximization 
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:Users/david/Dropbox/Papers/1506.06579v1.pdf:pdf},
journal = {International Conference on Machine Learning - Deep Learning Workshop 2015},
mendeley-groups = {Researh CSC2541 Project},
pages = {12},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}

% GoogleNet
@inproceedings{43022,
title = {Going Deeper with Convolutions},
author  = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
year  = 2015,
URL = {http://arxiv.org/abs/1409.4842},
booktitle = {Computer Vision and Pattern Recognition (CVPR)}
}

% GAN
@ARTICLE{2014arXiv1406.2661G,
   author = {{Goodfellow}, I.~J. and {Pouget-Abadie}, J. and {Mirza}, M. and 
	{Xu}, B. and {Warde-Farley}, D. and {Ozair}, S. and {Courville}, A. and 
	{Bengio}, Y.},
    title = "{Generative Adversarial Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1406.2661},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2014,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1406.2661G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% VAE
@ARTICLE{2013arXiv1312.6114K,
   author = {{Kingma}, D.~P and {Welling}, M.},
    title = "{Auto-Encoding Variational Bayes}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1312.6114},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2013,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% DRAW
@ARTICLE{2015arXiv150204623G,
   author = {{Gregor}, K. and {Danihelka}, I. and {Graves}, A. and {Jimenez Rezende}, D. and 
	{Wierstra}, D.},
    title = "{DRAW: A Recurrent Neural Network For Image Generation}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1502.04623},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2015,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204623G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% GAN text to image
@ARTICLE{2016arXiv160505396R,
   author = {{Reed}, S. and {Akata}, Z. and {Yan}, X. and {Logeswaran}, L. and 
	{Schiele}, B. and {Lee}, H.},
    title = "{Generative Adversarial Text to Image Synthesis}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1605.05396},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
     year = 2016,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160505396R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

% DRAW with Attention
@ARTICLE{2015arXiv151102793M,
   author = {{Mansimov}, E. and {Parisotto}, E. and {Lei Ba}, J. and {Salakhutdinov}, R.
	},
    title = "{Generating Images from Captions with Attention}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.02793},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151102793M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
